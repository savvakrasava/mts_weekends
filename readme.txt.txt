! Для запуска *.py необходимо предустановить библиотеки: json, sqlite3, string
! Версия python : 3.6


1. Написать скрипт на Python который загружает в БД (sqlite3) данные по каждому твиту из файла “three_minutes_tweets.json.txt”:
структура твита: name, tweet_text, country_code, display_url, lang, created_at, location

Методы:
createRawDataTable()  - создание структуры таблицы
insertRawData(filename) - заполнение таблицы 


2. Для каждого твита в базе необходимо завести атрибут с эмоциональной оценкой сообщения: tweet_sentiment

Метод:
addSentimentColumn()

3. Подумать как можно нормализовать хранение твита (привести SQL-скрипты создания/изменения нормализованной структуры данных)

Методы:
createNormalizedTables() - создание структуры таблиц
fillUsersTable() - заполнение таблицы 
fillLanguagesTable() - заполнение таблицы 
fillCounriesTable() - заполнение таблицы 

4. Написать скрипт на Python для подсчета среднего sentiment (Эмоциональной окраски сообщения) на основе AFINN-111.txt и заполнить  для каждого твита tweet_sentiment колонку. 
Если твит не содержит слов из словаря то предполагать что sentiment =0

Метод:
createSentimentDict(filename) - загрузка справочника
calculateTweetSentiment(phrase) - расчёт эмоциональной окраски


5. Написать SQL скрипт, который выводит наиболее и наименее счастливую страну, локацию и пользователя (дял пользователя - вместе с его твитами), предоставить сами скрипты и результаты их работы.

Метод:
printStatistics()

6. Описать свое видение решения, которое позволит выполнять ежедневно анализ согласно п.5. Из каких компонентов должно состоять решение, из каких шагов должен состоять ETL процесс от обработки входящих файлов до этапа сохранения конечной информации.
- на входе - непрерывный поток на FTP твитов в файлах (tweet.json), с частотой каждые три минуты. Размеры файлов - в среднем x10 от предоставленного сэмпла.
- на выходе - пользователи должны иметь возможность анализировать счастье по странам, локациям, пользователям, отслеживать изменения, собирать статиcтику и т.д. 

Сами файлы as is загружаются быстро. Увелечение объёмов в разы особой нагрузки не добавит.
Узкое место в постпроцессинге - при расчёте эмоциональной окраски твита.
Следовательно один из возможных вариантов, это использование hadoop ecosystem:
 1. Обработка файлов должна происходить на hdfs слое, (N машин(кластеров) должны обрабатывать файлы, если обработка json занимает менее 3ёх минут, значит нам нужны минимум 2 машины (отказоустойчивость), если более (и данные 
	нужны нам в "онлайне") то и кол-во машин будет больше).
 2. Обработка поступивших данных должна происходить в MapReduce(data processing) слое. Т.е. собирается статистика, какие-то агрегаты и т.д. по новым данным -> добавляется к уже имеющийся 
 3. После пункта 2 данные попадают уже в Data access слой, или data intelligence. Пользователи видят в онлайне новые данные, могут ими пользоваться.

Верхнеуровневую архитектуру вложил файлом: 6_arch_example.jpg

 